import OpenAI from 'openai';
import fs from 'fs';

const openai = new OpenAI({ 
  apiKey: process.env.OPENAI_API_KEY
});

const ASSISTANT_ID = "asst_JerNOWvyU63gex8p0z3gSv8r";

/**
 * Cr√©e un chunk sp√©cialis√© pour les discussions sur les mod√®les de juillet 2025
 */
async function createJulyModelsChunk() {
  try {
    const whatsappFile = './attached_assets/Discussion WhatsApp avec üîÅAi-Dialogue Actif_1752670591921.txt';
    const content = fs.readFileSync(whatsappFile, 'utf-8');
    
    const lines = content.split('\n');
    const julyModelsContent = [];
    
    // Extraire sp√©cifiquement les discussions sur les mod√®les en juillet 2025
    for (let i = 0; i < lines.length; i++) {
      const line = lines[i];
      
      // Si c'est une ligne de juillet ET qu'elle mentionne des mod√®les/IA
      if (line.includes('/07/2025') && 
          (line.toLowerCase().includes('mod√®le') || 
           line.toLowerCase().includes('model') ||
           line.toLowerCase().includes('mistral') ||
           line.toLowerCase().includes('gemini') ||
           line.toLowerCase().includes('claude') ||
           line.toLowerCase().includes('deepseek') ||
           line.toLowerCase().includes('google') ||
           line.toLowerCase().includes('gpt') ||
           line.toLowerCase().includes('llama') ||
           line.toLowerCase().includes('n3') ||
           line.toLowerCase().includes('r1') ||
           line.toLowerCase().includes('ollama') ||
           line.toLowerCase().includes('suisse') ||
           line.toLowerCase().includes('swiss'))) {
        
        // Ajouter le contexte autour
        const contextStart = Math.max(0, i - 8);
        const contextEnd = Math.min(lines.length, i + 8);
        const context = lines.slice(contextStart, contextEnd);
        
        julyModelsContent.push('=== DISCUSSION MOD√àLES JUILLET 2025 ===');
        julyModelsContent.push(...context);
        julyModelsContent.push('');
      }
    }
    
    // Ajouter sp√©cifiquement la discussion du 13/07/2025 sur les mod√®les Google N3 et DeepSeek R1
    const specificDiscussion = `
=== DISCUSSION SP√âCIALIS√âE: NOUVEAUX MOD√àLES JUILLET 2025 ===
Date: 13 juillet 2025
Participants: +32 483 60 53 96, Jean-Marc Everard, +33 6 66 98 84 23

CONTEXTE: Discussion sur les mod√®les recommand√©s pour les institutions

CONVERSATION:
13/07/2025, 11:53 - +32 483 60 53 96: Le mieux est de d√©ployer un mod√®le au niveau de l'institution ou partag√©e entre les institutions pour un public pr√©cis (exemple chercheurs ou administration) : comme par exemple un modele petit ollama ou modele Mistral small recent ; ya aussi le mod√®le Google N3 qui recemment est int√©ressant √† utiliser en local ; sinon un mod√®le de raisonnement deepseek R1 en local est un bon choix parfois en local (cest ce que j'ai vu de plus utilis√© dans les universit√©s)

13/07/2025, 16:46 - Jean-Marc Everard: Merci Robin. Quand j'essaye de mettre en place des syst√®mes et mod√®les locaux, les informaticiens m'opposent souvent la difficult√© du suivi, des mises √† jours des ¬´ containers ¬ª ou appli, des mod√®les, la mise en place d'une m√©moire etc‚Ä¶ est-ce que tu as d√©j√† √©t√© confront√© √† cela et envisag√© des modes op√©ratoires par rapport √† √ßa?

13/07/2025, 17:54 - +33 6 66 98 84 23: Bonjour, il me semble qu'il √©tait possible de louer un serveur sur lequel on pouvait d√©ployer une IA (comme en local) et l√† il n'y avait pas de pb de puissance ?

MOD√àLES MENTIONN√âS:
- Google N3 (r√©cemment int√©ressant en local)
- DeepSeek R1 (mod√®le de raisonnement, tr√®s utilis√© dans les universit√©s)
- Mistral Small (version r√©cente)
- Ollama (mod√®le petit pour institutions)

MOTS-CL√âS: Google N3, DeepSeek R1, Mistral, Ollama, juillet 2025, mod√®les r√©cents, institutions, universit√©s
`;

    const finalContent = julyModelsContent.join('\n') + specificDiscussion;
    
    console.log('=== CHUNK MOD√àLES JUILLET 2025 ===');
    console.log(`Contenu captur√©: ${julyModelsContent.length} lignes`);
    console.log(`Taille totale: ${finalContent.length} caract√®res`);
    
    // Cr√©er le fichier sp√©cialis√©
    const fileBuffer = Buffer.from(finalContent, 'utf-8');
    
    const file = await openai.files.create({
      file: new File([fileBuffer], 'July2025_AI_Models_Discussion.txt', { type: 'text/plain' }),
      purpose: 'assistants'
    });
    
    // R√©cup√©rer le Vector Store actuel
    const assistant = await openai.beta.assistants.retrieve(ASSISTANT_ID);
    const vectorStoreId = assistant.tool_resources?.file_search?.vector_store_ids?.[0];
    
    if (vectorStoreId) {
      // Ajouter le fichier au Vector Store
      await openai.beta.vectorStores.files.create(vectorStoreId, {
        file_id: file.id
      });
      
      console.log('‚úÖ Chunk mod√®les juillet 2025 cr√©√© et ajout√©');
      console.log(`File ID: ${file.id}`);
      console.log(`Vector Store: ${vectorStoreId}`);
      
      console.log('\n=== MOD√àLES IDENTIFI√âS EN JUILLET ===');
      console.log('- Google N3 (r√©cemment int√©ressant)');
      console.log('- DeepSeek R1 (raisonnement, universit√©s)');
      console.log('- Mistral Small (version r√©cente)');
      console.log('- Ollama (pour institutions)');
      
      return {
        success: true,
        fileId: file.id,
        vectorStoreId: vectorStoreId,
        contentLines: julyModelsContent.length
      };
    } else {
      throw new Error('Vector Store non trouv√©');
    }
    
  } catch (error) {
    console.error('Erreur cr√©ation chunk juillet:', error);
    return { success: false, error: error.message };
  }
}

export { createJulyModelsChunk };