=== MÃ‰TADONNÃ‰ES CHUNK ===
PÃ©riode: null â†’ null
Participants: 
Topics: 
Tokens: 566
Messages: 11

=== CONTENU ===
30/05/2024, 19:29 - Rochane: Perso, j'ai jamais testÃ© car je n'ai pas de machine puissante et je ne suis pas sur windows mais curieux d'avoir des retours d'expÃ©rience ğŸ™‚
30/05/2024, 19:39 - Pierrick BRIAND: Bonsoir @33625795095
Tu dois parler de Phi-3 Mini sur smartphone. Câ€™est clair quâ€™un modÃ¨le local offre en thÃ©orie plus de contrÃ´le et plus de confidentialitÃ© Ã  l'usage, mais il hÃ©rite aussi en partie des caractÃ©ristiques de ses donnÃ©es d'entraÃ®nement, qui restent opaques. Les donnÃ©es utilisÃ©es pour entraÃ®ner le modÃ¨le en amont soulÃ¨vent pas mal de questions car Phi-3 a Ã©tÃ© entraÃ®nÃ© sur des Â«Â donnÃ©es web fortement filtrÃ©esÂ Â» et des Â«Â donnÃ©es synthÃ©tiquesÂ Â». Si tu peux tester ce mini LLM et partager tes retours, ce serait une excellente idÃ©e !
30/05/2024, 21:24 - +33 6 81 36 11 13: Bonsoir , voici un lien vers une question sur Perplexity. comment je peux prÃ©venir l'IA que sa rÃ©ponse est fausse? Lien : https://www.perplexity.ai/search/Bonjour-Je-suis-UFYJ3Uw8Q4CcTuz9Nf3nlw
30/05/2024, 21:27 - Rochane: C'est connu que l'IA n'est pas forte en maths mÃªme si Gpt 4o commence Ã  Ãªtre meilleur. Tu as testÃ© Gpt 4o ( c'est gratuit) ? Et donne lui un rÃ´le de prof de maths avec une grande expÃ©rience ğŸ™‚
30/05/2024, 21:29 - Rochane: LÃ  il a trouvÃ© ğŸ™‚
https://chatgpt.com/share/710ce708-652d-4c6c-a31e-908e63ae8f97
30/05/2024, 21:29 - +33 6 81 36 11 13: c'est un de mes Ã©lÃ¨ves qui teste Perplexity pour moi sur l'Ã©criture des prompt. je le fait avec ma classe sur chatGPT en fin de mois en 4Ã¨me. Sa question est : Comment faire comprendre Ã  l'IA qu'il s'est trompÃ©?. j'avoue que je ne sais pas quoi lui dire
30/05/2024, 21:31 - +33 6 81 36 11 13: merci, je vois bien le rÃ´le de chatGPT. Je vais bien m'amuser avec mes 4Ã¨me je crois.
30/05/2024, 21:32 - LIO: Ce message a Ã©tÃ© supprimÃ©.
31/05/2024, 10:37 - +33 6 25 79 50 95: Merci Pierrick pour ton Ã©clairage sur l'hÃ©ritage du jeu (sÃ©lectif) de donnÃ©es ayant servi Ã  entraÃ®ner Phi-3. La taille du LLM est une chose mais la qualitÃ© des donnÃ©es d'entraÃ®nement en est un autre, en effet. Par ailleurs je me doute que la solution locale sera moins satisfaisante que les LLM en ligne, mais l'idÃ©e d'une confidentialitÃ© (thÃ©orique) tout en contribuant Ã  une forme de sobriÃ©tÃ© digitale donne envie d'essayer.